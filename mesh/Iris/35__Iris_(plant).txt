K-MEANS | Projects
Projects
LOG IN
User Name or EMail:
*
Password:
*
Forgot your Password?
OR
New to Classle? Sign Up  here!
Already have an account?Sign in  here!
Skip to content
K-MEANS
Description To-Do Lists Messages Milestones Wiki Pages People
You are not Member of this Project. Request membership
Project Owner :
Shyam.C
Created Date :
Thu, 15/03/2012 - 22:54
Project Description :
In  data mining ,  k -means clustering  is a method of  cluster analysis  which aims to  partition   n  observations into  k  clusters in which each observation belongs to the cluster with the nearest  mean . This results into a partitioning of the data space into  Voronoi cells .
The problem is computationally difficult ( NP-hard ), however there are efficient  heuristic algorithms  that are commonly employed that converge fast to a local optimum. These are usually similar to the  expectation-maximization algorithm  for  mixtures  of  Gaussian distributions  via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data, however  k -means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.
Standard algorithm
The most common algorithm uses an iterative refinement technique. Due to its ubiquity it is often called the  k -means algorithm ; it is also referred to as  Lloyd's algorithm , particularly in the computer science community.
Given an initial set of  k  means  m 1 (1) ,…, m k (1)  (see below), the algorithm proceeds by alternating between two steps: [10]
Assignment step : Assign each observation to the cluster with the closest mean (i.e. partition the observations according to the  Voronoi diagram  generated by the means).
Where each   goes into exactly one  , even if it could go in two of them.
Update step : Calculate the new means to be the centroid of the observations in the cluster.
The algorithm is deemed to have converged when the assignments no longer change.
Commonly used initialization methods are Forgy and Random Partition.
The Forgy method randomly chooses  k  observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the Update step, thus computing the initial means to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable.
The two key features of  k -means which make it efficient are often regarded as its biggest drawbacks:
Euclidean distance  is used as a  metric  and  variance  is used as a measure of cluster scatter.
The number of clusters  k  is an input parameter: an inappropriate choice of  k  may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for  determining the number of clusters in the data set .
A key limitation of  k -means is its cluster model. The concept is based on spherical clusters that are separable in a way so that the mean value converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying  k -means with a value of   onto the well-known  Iris flower data set , the result often fails to separate the three  Iris  species contained in the data set. With  , the two visible clusters (one containing two species) will be discovered, whereas with  one of the two clusters will be split into two even parts. In fact,   is more appropriate for this data set, despite the data set containing 3  classes . As with any other clustering algorithm, the  k -means result relies on the data set to satisfy the assumptions made by the clustering algorithms. It works very well on some data sets, while failing miserably on others.
The result of  k -means can also be seen as the  Voronoi cells  of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the "mouse" example. The Gaussian models used by the  Expectation-maximization algorithm  (which can be seen as a generalization of  k -means) are more flexible here by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than  k -means as well as correlated clusters (not in this example).
Applications of the algorithm
k -means clustering in particular when using heuristics such as Lloyd's algorithm is rather easy to implement and apply even on large data sets. As such, it has been successfully used in various topics, ranging from  market segmentation ,  computer vision ,  geostatistics . [21]  and  astronomy  to agriculture . It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.
Relation to other statistical machine learning algorithms
k -means clustering, and its associated expectation-maximization algorithm, is a special case of a  Gaussian mixture model , specifically, the limit of taking all covariances as diagonal, equal, and small. It is often easy to generalize a  k -means problem into a Gaussian mixture model.
Mean shift clustering
Basic  mean shift  clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast,  k -means restricts this updated set to  k  points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the  input set  that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to  k -means, called  likelihood mean shift , replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. [23]  One of the advantages of mean shift over  k -means is that there is no need to choose the number of clusters, because mean shift is likely to find only a few clusters if indeed only a small number exist. However, mean shift can be much slower than  k -means. Mean shift has soft variants much as  k -means does.
Principal Components Analysis (PCA)
It has been shown [24] [25]  that the relaxed solution of  k -means clustering, specified by the cluster indicators, is given by the PCA ( principal component analysis ) principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace.
Bilateral filtering
k -means implicitly assumes that the ordering of the input data set does not matter. The  bilateral filter  is similar to K-means and  mean shift  in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data.
This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.
You are not authorized to access this content. You are not authorized to access this content. You are not authorized to access this content. You are not authorized to access this content. You are not authorized to access this content.
ABOUT CLASSLE
Our Mission
Why Classle
Our Team
Corporate
Features
Services
Jobs
MEDIA
Media Releases
Case Studies
Press Kit
CONTACT
Contact Us
Careers
HELP
FAQ
Report a Problem
SOCIAL
Facebook
Twitter
LinkedIn
Google+
Blog
Desktop Version available for
Windows Ubuntu
© Copyright Classle. All Rights Reserved.
Privacy
|
Terms
|
Disclaimer